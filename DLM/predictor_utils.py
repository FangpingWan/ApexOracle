"""Utility functions and classes for training noise-conditioned predictors.

This module contains:
- Model wrappers (mol_emb_mdlm)
- Dataset classes (SMILESDataset_with_genome_and_text, SMILESDataset_with_text_only)
- Data processing functions
- Collate functions for DataLoader
- Helper functions for clustering and mapping
- Embedding loading functions (load_all_genome_embeddings, load_text_wo_genome_embeddings)

Note: RegressionHead and FirstTokenAttention_genome are imported from apexoracle_layers.py to avoid duplication.
"""

from pathlib import Path
import argparse
import json
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModel, AutoTokenizer
from sklearn.model_selection import KFold
from sklearn.metrics import average_precision_score
from torch.nn.utils.rnn import pad_sequence
from sklearn.cluster import AgglomerativeClustering
from Bio import Phylo
from triton.language import bfloat16
from scipy.stats import pearsonr, spearmanr
import json
import itertools
import logging

import hydra
from hydra import compose, initialize
import models
from collections import OrderedDict
import noise_schedule

import torch.nn.functional as F
import ast

# Import shared components from apexoracle_layers to avoid duplication
import apexoracle_layers
from apexoracle_layers import (
    RegressionHead,
    FirstTokenAttention_genome
)

# current_directory = Path(__file__).parent
current_directory = Path('/data2/tianang/projects/Synergy')

with initialize(config_path="configs"):
    config = compose(config_name="config")


class mol_emb_mdlm(nn.Module):
    """Wrapper for MDLM model to extract molecular embeddings with noise conditioning."""

    def __init__(self, config, vocab_size, ckpt_path, mask_index):
        super(mol_emb_mdlm, self).__init__()
        self.config = config
        self.vocab_size = vocab_size
        self.mask_index = mask_index
        self.ckpt_path = ckpt_path
        self.parameterization = self.config.parameterization
        self.time_conditioning = self.config.time_conditioning
        self.backbone = self.load_DIT()  # hidden_size = 768
        # print(self.bert.config.max_position_embeddings)
        self.noise = noise_schedule.get_noise(self.config)

    def _process_sigma(self, sigma):
        if sigma is None:
            assert self.parameterization == 'ar'
            return sigma
        if sigma.ndim > 1:
            sigma = sigma.squeeze(-1)
        if not self.time_conditioning:
            sigma = torch.zeros_like(sigma)
        assert sigma.ndim == 1, sigma.shape
        return sigma

    def _sample_t(self, n, device):
        sampling_eps = 1e-3
        _eps_t = torch.rand(n, device=device)  # * 0
        t = (1 - sampling_eps) * _eps_t + sampling_eps
        return t

    def _forward(self, x, sigma):
        sigma = self._process_sigma(sigma)
        with torch.cuda.amp.autocast(dtype=torch.float32):
            x = self.backbone.vocab_embed(x)
            c = F.silu(self.backbone.sigma_map(sigma))
            rotary_cos_sin = self.backbone.rotary_emb(x)

            with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                for i in range(len(self.backbone.blocks)):
                    x = self.backbone.blocks[i](x, rotary_cos_sin, c, seqlens=None)

        return x

    def q_xt(self, x, move_chance):
        """Computes the noisy sample xt.

        Args:
          x: int torch.Tensor with shape (batch_size,
              diffusion_model_input_length), input.
          move_chance: float torch.Tensor with shape (batch_size, 1).
        """
        move_indices = torch.rand(*x.shape, device=x.device) < move_chance
        xt = torch.where(move_indices, self.mask_index, x)

        # ---------------------------
        # 用于去掉 padding 部分的 mask

        # ---------------------------

        return xt

    def forward(self, input_ids, attention_mask=None):
        t = self._sample_t(input_ids.shape[0], input_ids.device)
        sigma, dsigma = self.noise(t)
        unet_conditioning = sigma[:, None]
        move_chance = 1 - torch.exp(-sigma[:, None])
        xt = self.q_xt(input_ids, move_chance)
        outputs = self._forward(xt, unet_conditioning)
        return outputs

    def load_DIT(self):
        backbone = models.dit.DIT(self.config, vocab_size=self.vocab_size)
        lightning_ckpt = torch.load(self.ckpt_path, map_location='cpu')
        state_dict = lightning_ckpt['state_dict']

        new_sd = OrderedDict()
        for k, v in state_dict.items():
            if k.startswith('backbone.'):
                new_key = k[len('backbone.'):]
            else:
                new_key = k
            new_sd[new_key] = v

        backbone.load_state_dict(new_sd, strict=False)

        return backbone


def get_embedded_genome_IDs(folder_path):
    """
    检查哪些 genome ID 的genome已经被转成 Evo2 的 embedding 了
    :param folder_path: 保存 Genome 的 Evo2 embed 的文件夹路径
    :return: 不带 ATCC 的纯 ID list  | e.g. ['25332', '11060', 'BAA-252', ...]
    """
    stored_genome_IDs = []
    genome_ID_to_species_first_name_dict = {}
    files = [f.name for f in folder_path.iterdir() if f.is_file()]
    for file_name in files:
        file_name = file_name.split('.')[0]
        file_name_temp = file_name.split('ATCC')[-1]
        components = file_name_temp.split('_')[1:]
        if len(components) == 2:
            ATCC_ID = '-'.join(components)
            stored_genome_IDs.append(ATCC_ID)  # 组装成形如 'BAA-252' 或者 'MYA-730'
        else:
            ATCC_ID = components[0]
            stored_genome_IDs.append(ATCC_ID)  # 就是普通的 '25922'

        genome_ID_to_species_first_name_dict[ATCC_ID] = file_name.split('_')[0]

    return stored_genome_IDs, genome_ID_to_species_first_name_dict


def get_original_strain_name_with_genome_embedding(Evo_MIC_count_file_path, embedded_genome_IDs):
    with open(Evo_MIC_count_file_path, 'r', encoding='utf-8') as f:
        strain_count_data = json.load(f)  # 解析 JSON 文件

    origin_to_standard_name_map_list_handcrafted = []  # [(original_name, standard_name (ATCC ID)), (Staphylococcus aureus ATCC 25923, 25923)...]
    origin_to_standard_name_map_list_DBAASP_original = []
    for name, count in strain_count_data.items():

        # 先处理手动标记的 strain
        if '*' in name:
            original_name, standard_name = name.split('*')
            if 'ATCC' in standard_name:
                standard_name = standard_name.split('ATCC')[-1].strip()
            else:
                # 包含那些没有 ATCC 但是单独下载了 Genome 数据的
                standard_name = standard_name.strip()
            origin_to_standard_name_map_list_handcrafted.append((original_name.strip(), standard_name))

        # 如果没有手动标记，那就只处理原始 strain 中就有 ATCC ID 的那些
        else:
            if 'ATCC' in name:
                original_name = name
                ATCC_id = name.split('ATCC')[-1].strip()
                if 'BAA' in name:
                    ATCC_id = ATCC_id.replace(" ", "-")
                if 'MY' in name:
                    ATCC_id = ATCC_id.replace(" ", "")
                if 'MAY' in name:
                    ATCC_id = ATCC_id.replace("MAY", "MYA")
                if 'D' in name:
                    ATCC_id = ATCC_id.split("D")[0]
                if 'T' in name:
                    ATCC_id = ATCC_id.split("T")[0]
                if 's' in name:
                    ATCC_id = ATCC_id.split("s")[0]
                if " " in name:
                    ATCC_id = ATCC_id.split(" ")[0]

                origin_to_standard_name_map_list_DBAASP_original.append((original_name.strip(), ATCC_id))

    origin_to_standard_name_map_list = np.array(origin_to_standard_name_map_list_handcrafted + origin_to_standard_name_map_list_DBAASP_original)

    original_names_with_genome_embedding_handcrafted = []  # 提取出那些有对应 Evo2 embedding 的 DBAASP 中的完整 strain name
    for line_idx, (original_name, standard_name) in enumerate(origin_to_standard_name_map_list_handcrafted):
        # 检查这些 ATCC ID 是不是已经在有 Evo2 embedding 的 strain 里
        if standard_name in embedded_genome_IDs:
            original_names_with_genome_embedding_handcrafted.append(original_name)

    original_names_with_genome_embedding_DBAASP_original = []  # 提取出那些有对应 Evo2 embedding 的 DBAASP 中的完整 strain name
    for line_idx, (original_name, standard_name) in enumerate(origin_to_standard_name_map_list_DBAASP_original):
        # 检查这些 ATCC ID 是不是已经在有 Evo2 embedding 的 strain 里
        if standard_name in embedded_genome_IDs:
            original_names_with_genome_embedding_DBAASP_original.append(original_name)

    return original_names_with_genome_embedding_handcrafted, original_names_with_genome_embedding_DBAASP_original, dict(origin_to_standard_name_map_list)


class SMILESDataset_with_genome_and_text(Dataset):
    """Dataset for molecular sequences with genome and text embeddings.

    Note: Despite the name 'SMILES', this dataset actually processes tokenized
    SELFIES (SELF-referencIng Embedded Strings) representations, not SMILES.
    The 'SMILES' column contains pre-tokenized SELFIES strings.
    """

    def __init__(self, dataframe, tokenizer, embeddings_dict, text_embeddings_dict, set_desc:str, max_length=1024):
        self.dataframe = dataframe
        self.original_length = len(self.dataframe)
        self.tokenizer = tokenizer
        self.embeddings_dict = embeddings_dict
        self.text_embeddings_dict = text_embeddings_dict
        self.max_length = max_length
        self.target_columns = 'MIC'
        self.remove_long_smiles()
        # print(f'\n {set_desc}:\n original length: {self.original_length}\n after SMILES length limitation length: {len(self.dataframe)}')
        logging.getLogger().info(f'\n {set_desc}:\n original length: {self.original_length}\n after SMILES length limitation length: {len(self.dataframe)}')

    def tokenize_smiles(self, smiles):
        """Process pre-tokenized SELFIES string.

        Note: The parameter is named 'smiles' for historical reasons, but it
        actually contains pre-tokenized SELFIES representations stored as strings.
        """
        # Parse pre-tokenized SELFIES string and return input_ids and attention_mask
        input_ids = torch.from_numpy(np.array(ast.literal_eval(smiles)))
        # input_ids = tokenized['input_ids'].squeeze(0)
        attn_mask = torch.ones_like(input_ids)
        return input_ids, attn_mask

    def remove_long_smiles(self):
        # self.dataframe = self.dataframe[self.dataframe['SMILES'].apply(lambda x: len(self.tokenizer(x, return_tensors='pt', padding=False, truncation=False)['input_ids'].squeeze(0)) <= self.max_length)]
        # self.dataframe = self.dataframe.reset_index(drop=True)  # 重置索引

        # 对 SMILES 列进行 tokenize，并拆分为两列
        tokenized_cols = self.dataframe['SMILES'].apply(
            lambda x: pd.Series(self.tokenize_smiles(x), index=['input_ids', 'attn_mask'])
        )

        # 将新的两列拼接到原 dataframe 中
        self.dataframe = pd.concat([self.dataframe, tokenized_cols], axis=1)

        # 根据 input_ids 长度进行过滤，确保 token 长度不超过 max_length
        self.dataframe = self.dataframe[self.dataframe['input_ids'].apply(len) <= self.max_length]
        self.dataframe = self.dataframe.reset_index(drop=True)

        # 删除原来的 SMILES 列
        self.dataframe.drop(columns=['SMILES'], inplace=True)

        # self.dataframe.to_csv('/home/tianang/Projects/Synergy/DataPrepare/Data/DBAASP_id_SMILES_bact_MICs_512_limit.csv', index=False)
        # print(f'new data file saved to /home/tianang/Projects/Synergy/DataPrepare/Data/DBAASP_id_SMILES_bact_MICs_512_limit.csv')

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        # smiles = self.dataframe.iloc[idx]['SMILES']
        # DBAASP_id = self.dataframe.iloc[idx]['DBAASP_id']
        # target_columns = self.dataframe.columns.tolist()[2:]
        strain_name = self.dataframe.iloc[idx]['strain_name']
        target = self.dataframe.iloc[idx][self.target_columns]
        # inputs = self.tokenizer(smiles, return_tensors='pt', padding=False, truncation=False)  #, max_length=self.max_length)
        # inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # 去掉 batch 维度
        return {
            'input_ids': self.dataframe.iloc[idx]['input_ids'],
            'attention_mask': self.dataframe.iloc[idx]['attn_mask'],
            'label': torch.tensor(target, dtype=torch.float),
            'genome_embedding': self.embeddings_dict[strain_name],
            'text_embedding': self.text_embeddings_dict[strain_name],
            'strain_name': strain_name
        }


class SMILESDataset_with_text_only(SMILESDataset_with_genome_and_text):
    """Dataset for molecular sequences with text embeddings only (no genome data).

    Note: Despite the name 'SMILES', this dataset actually processes tokenized
    SELFIES (SELF-referencIng Embedded Strings) representations, not SMILES.
    """

    def __init__(self, dataframe, tokenizer, text_embeddings_dict, set_desc: str, max_length=1024):
        # 调用父类的 __init__ 方法时，可以将 embeddings_dict 传入一个 None 或者空字典（如果父类内部没有用到的话）
        super().__init__(dataframe, tokenizer, embeddings_dict=None, text_embeddings_dict=text_embeddings_dict, set_desc=set_desc, max_length=max_length)
        # 如果父类中对 self.embeddings_dict 有特殊处理，可以在这里重置或忽略它

    def __getitem__(self, idx):
        strain_name = self.dataframe.iloc[idx]['strain_name']
        target = self.dataframe.iloc[idx][self.target_columns]
        return {
            'input_ids': self.dataframe.iloc[idx]['input_ids'],
            'attention_mask': self.dataframe.iloc[idx]['attn_mask'],
            'label': torch.tensor(target, dtype=torch.float),
            'text_embedding': self.text_embeddings_dict[strain_name],
            'strain_name': strain_name
        }


# Global tokenizer reference - will be set by training script
_tokenizer = None

def set_tokenizer(tokenizer):
    """Set global tokenizer for collate functions."""
    global _tokenizer
    _tokenizer = tokenizer

def collate_fn(batch):
    """
    这里把一个batch中所有的label都转换成 log 计算之后的
    """
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    labels = [item['label'] for item in batch]
    genome_embeddings = [item['genome_embedding'] for item in batch]
    text_embeddings = [item['text_embedding'] for item in batch]
    strain_names = [item['strain_name'] for item in batch]

    max_genome_length = 0
    for genome_embedding in genome_embeddings:
        if len(genome_embedding) > max_genome_length:
            max_genome_length = len(genome_embedding)

    padded_genome_embeddings = []
    genome_attn_masks = []
    for genome_embedding in genome_embeddings:
        L,D = genome_embedding.shape
        genome_attn_mask = torch.zeros(max_genome_length, device=genome_embedding.device, dtype=torch.uint8)
        genome_padding = torch.zeros((max_genome_length, D), dtype=torch.bfloat16, device=genome_embedding.device)
        genome_padding[:L] = genome_embedding
        genome_attn_mask[:L] = 1
        padded_genome_embeddings.append(genome_padding)
        genome_attn_masks.append(genome_attn_mask)

    padded_genome_embeddings = torch.stack(padded_genome_embeddings)
    genome_attn_masks = torch.stack(genome_attn_masks)

    max_text_length = 0
    for text_embedding in text_embeddings:
        if len(text_embedding) > max_text_length:
            max_text_length = len(text_embedding)

    padded_text_embeddings = []
    text_attn_masks = []
    for text_embedding in text_embeddings:
        L, D = text_embedding.shape
        text_attn_mask = torch.zeros(max_text_length, device=text_embedding.device, dtype=torch.uint8)
        text_padding = torch.zeros((max_text_length, D), dtype=torch.bfloat16, device=text_embedding.device)
        text_padding[:L] = text_embedding
        text_attn_mask[:L] = 1
        padded_text_embeddings.append(text_padding)
        text_attn_masks.append(text_attn_mask)

    padded_text_embeddings = torch.stack(padded_text_embeddings)
    text_attn_masks = torch.stack(text_attn_masks)

    # 使用 pad_sequence 填充输入
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=_tokenizer.pad_token_id)
    padded_input_ids = torch.ones([len(input_ids), 1024], dtype=input_ids.dtype) * _tokenizer.pad_token_id
    padded_input_ids[:, :input_ids.shape[-1]] = input_ids
    input_ids = padded_input_ids

    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
    padded_attention_mask = torch.zeros([len(input_ids), 1024], dtype=input_ids.dtype)
    padded_attention_mask[:, :attention_mask.shape[-1]] = attention_mask
    attention_mask = padded_attention_mask
    labels = torch.from_numpy(np.array(labels))
    # mask = labels >= -0.5  # 生成多任务回归使用的 label mask
    # labels_processed = labels.clone()  # 复制原张量以保留未满足条件的值

    # 计算实际的用来回归的值
    labels = -torch.log10(labels / 10)

    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'label': labels,
        'padded_genome_embeddings': padded_genome_embeddings,
        'genome_attn_masks': genome_attn_masks,
        'padded_text_embeddings': padded_text_embeddings,
        'text_attn_masks': text_attn_masks,
        'strain_names': strain_names
    }


def collate_fn_text_only(batch):
    """
    这里把一个batch中所有的label都转换成 log 计算之后的
    """
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    labels = [item['label'] for item in batch]
    # genome_embeddings = [item['genome_embedding'] for item in batch]
    text_embeddings = [item['text_embedding'] for item in batch]
    strain_names = [item['strain_name'] for item in batch]

    max_text_length = 0
    for text_embedding in text_embeddings:
        if len(text_embedding) > max_text_length:
            max_text_length = len(text_embedding)

    padded_text_embeddings = []
    text_attn_masks = []
    for text_embedding in text_embeddings:
        L, D = text_embedding.shape
        text_attn_mask = torch.zeros(max_text_length, device=text_embedding.device, dtype=torch.uint8)
        text_padding = torch.zeros((max_text_length, D), dtype=torch.bfloat16, device=text_embedding.device)
        text_padding[:L] = text_embedding
        text_attn_mask[:L] = 1
        padded_text_embeddings.append(text_padding)
        text_attn_masks.append(text_attn_mask)

    padded_text_embeddings = torch.stack(padded_text_embeddings)
    text_attn_masks = torch.stack(text_attn_masks)

    # 使用 pad_sequence 填充输入
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=_tokenizer.pad_token_id)
    padded_input_ids = torch.ones([len(input_ids), 1024], dtype=input_ids.dtype) * _tokenizer.pad_token_id
    padded_input_ids[:, :input_ids.shape[-1]] = input_ids
    input_ids = padded_input_ids

    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
    padded_attention_mask = torch.zeros([len(input_ids), 1024], dtype=input_ids.dtype)
    padded_attention_mask[:, :attention_mask.shape[-1]] = attention_mask
    attention_mask = padded_attention_mask
    labels = torch.from_numpy(np.array(labels))
    # mask = labels >= -0.5  # 生成多任务回归使用的 label mask
    # labels_processed = labels.clone()  # 复制原张量以保留未满足条件的值

    # 计算实际的用来回归的值
    labels = -torch.log10(labels / 10)

    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'label': labels,
        'padded_text_embeddings': padded_text_embeddings,
        'text_attn_masks': text_attn_masks,
        'strain_names': strain_names
    }


def collate_fn_cls(batch):
    """
    这里把一个batch中所有的label都转换成 log 计算之后的
    """
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    labels = [item['label'] for item in batch]
    genome_embeddings = [item['genome_embedding'] for item in batch]
    text_embeddings = [item['text_embedding'] for item in batch]
    strain_names = [item['strain_name'] for item in batch]

    max_genome_length = 0
    for genome_embedding in genome_embeddings:
        if len(genome_embedding) > max_genome_length:
            max_genome_length = len(genome_embedding)

    padded_genome_embeddings = []
    genome_attn_masks = []
    for genome_embedding in genome_embeddings:
        L,D = genome_embedding.shape
        genome_attn_mask = torch.zeros(max_genome_length, device=genome_embedding.device, dtype=torch.uint8)
        genome_padding = torch.zeros((max_genome_length, D), dtype=torch.bfloat16, device=genome_embedding.device)
        genome_padding[:L] = genome_embedding
        genome_attn_mask[:L] = 1
        padded_genome_embeddings.append(genome_padding)
        genome_attn_masks.append(genome_attn_mask)

    padded_genome_embeddings = torch.stack(padded_genome_embeddings)
    genome_attn_masks = torch.stack(genome_attn_masks)

    max_text_length = 0
    for text_embedding in text_embeddings:
        if len(text_embedding) > max_text_length:
            max_text_length = len(text_embedding)

    padded_text_embeddings = []
    text_attn_masks = []
    for text_embedding in text_embeddings:
        L, D = text_embedding.shape
        text_attn_mask = torch.zeros(max_text_length, device=text_embedding.device, dtype=torch.uint8)
        text_padding = torch.zeros((max_text_length, D), dtype=torch.bfloat16, device=text_embedding.device)
        text_padding[:L] = text_embedding
        text_attn_mask[:L] = 1
        padded_text_embeddings.append(text_padding)
        text_attn_masks.append(text_attn_mask)

    padded_text_embeddings = torch.stack(padded_text_embeddings)
    text_attn_masks = torch.stack(text_attn_masks)

    # 使用 pad_sequence 填充输入
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=_tokenizer.pad_token_id)
    padded_input_ids = torch.ones([len(input_ids), 1024], dtype=input_ids.dtype) * _tokenizer.pad_token_id
    padded_input_ids[:, :input_ids.shape[-1]] = input_ids
    input_ids = padded_input_ids

    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
    padded_attention_mask = torch.zeros([len(input_ids), 1024], dtype=input_ids.dtype)
    padded_attention_mask[:, :attention_mask.shape[-1]] = attention_mask
    attention_mask = padded_attention_mask
    labels = torch.from_numpy(np.array(labels))
    # mask = labels >= -0.5  # 生成多任务回归使用的 label mask
    # labels_processed = labels.clone()  # 复制原张量以保留未满足条件的值

    # 计算实际的用来回归的值
    # labels = -torch.log10(labels / 10)

    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'label': labels,
        'padded_genome_embeddings': padded_genome_embeddings,
        'genome_attn_masks': genome_attn_masks,
        'padded_text_embeddings': padded_text_embeddings,
        'text_attn_masks': text_attn_masks,
        'strain_names': strain_names
    }


def collate_fn_text_only_cls(batch):
    """
    这里把一个batch中所有的label都转换成 log 计算之后的
    """
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    labels = [item['label'] for item in batch]
    # genome_embeddings = [item['genome_embedding'] for item in batch]
    text_embeddings = [item['text_embedding'] for item in batch]
    strain_names = [item['strain_name'] for item in batch]

    max_text_length = 0
    for text_embedding in text_embeddings:
        if len(text_embedding) > max_text_length:
            max_text_length = len(text_embedding)

    padded_text_embeddings = []
    text_attn_masks = []
    for text_embedding in text_embeddings:
        L, D = text_embedding.shape
        text_attn_mask = torch.zeros(max_text_length, device=text_embedding.device, dtype=torch.uint8)
        text_padding = torch.zeros((max_text_length, D), dtype=torch.bfloat16, device=text_embedding.device)
        text_padding[:L] = text_embedding
        text_attn_mask[:L] = 1
        padded_text_embeddings.append(text_padding)
        text_attn_masks.append(text_attn_mask)

    padded_text_embeddings = torch.stack(padded_text_embeddings)
    text_attn_masks = torch.stack(text_attn_masks)

    # 使用 pad_sequence 填充输入
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=_tokenizer.pad_token_id)
    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = torch.from_numpy(np.array(labels))
    # mask = labels >= -0.5  # 生成多任务回归使用的 label mask
    # labels_processed = labels.clone()  # 复制原张量以保留未满足条件的值

    # 计算实际的用来回归的值
    # labels = -torch.log10(labels / 10)

    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'label': labels,
        'padded_text_embeddings': padded_text_embeddings,
        'text_attn_masks': text_attn_masks,
        'strain_names': strain_names
    }


class ClsHead(nn.Module):
    """Head for sentence-level classification tasks."""

    def __init__(
            self,
            input_dim,
            hidden_dim_1=384,
            hidden_dim_2=128,
            num_targets=19,
            pooler_dropout: float = 0.2,
    ):
        """
        Initialize the classification head.

        :param input_dim: Dimension of input features.
        :param inner_dim: Dimension of the inner layer.
        :param num_classes: Number of classes for classification.
        :param activation_fn: Activation function name.
        :param pooler_dropout: Dropout rate for the pooling layer.
        """
        super().__init__()
        self.dense_1 = nn.Linear(input_dim, hidden_dim_1)
        self.dense_2 = nn.Linear(hidden_dim_1, hidden_dim_2)
        self.activation_fn = nn.GELU()
        self.dropout = nn.Dropout(p=pooler_dropout)
        self.out_proj = nn.Linear(hidden_dim_2, num_targets)

    def forward(self, features, **kwargs):
        """
        Forward pass for the classification head.

        :param features: Input features for classification.

        :return: Output from the classification head.
        """
        x = self.dense_1(features)
        x = self.activation_fn(x)
        x = self.dropout(x)

        x = self.dense_2(x)
        x = self.activation_fn(x)
        x = self.dropout(x)

        x = self.out_proj(x)
        # x = torch.sigmoid(x)
        return x


def calculate_r2(all_labels, all_preds):
    # 确保输入是 numpy 数组
    all_labels = np.array(all_labels)
    all_preds = np.array(all_preds)

    # 计算 R^2
    ss_total = np.sum((all_labels - np.mean(all_labels)) ** 2)  # 总平方和
    ss_residual = np.sum((all_labels - all_preds) ** 2)  # 残差平方和
    r2 = 1 - (ss_residual / ss_total)

    return r2


def exclude_wrong_species_ATCC_map(Evo_MIC_data_with_genome_embedding:np.array, genome_ID_to_species_first_name_dict):
    """
    去掉那些原始 DBAASP 中连 species name 和 ATCC ID 都对不上的数据，只处理那些没有手动标注的！
    :param Evo_MIC_data_with_genome_embedding: SMIELS, strain -> MIC data
    :param genome_ID_to_species_first_name_dict: dict, {ATCC_ID: species_name }, 这个是直接从 保存的 ATCC genome embedding 文件名获得的
    :return: cleaned SMIELS, strain -> MIC data, np.array
    """
    # 记录一下清理之前有多少数据点
    original_length = len(Evo_MIC_data_with_genome_embedding)

    marked_ATCC_IDs = set()
    cleaned_data = []
    for line in Evo_MIC_data_with_genome_embedding:
        name = line[1]

        # 那些没有 ATCC ID 但是一定被手动标注了的情况
        if 'ATCC' not in name:
            cleaned_data.append(line)
            continue

        if 'ATCC' in name:
            ATCC_id = name.split('ATCC')[-1].strip()
            if 'BAA' in name:
                ATCC_id = ATCC_id.replace(" ", "-")
            if 'MY' in name:
                ATCC_id = ATCC_id.replace(" ", "")
            if 'MAY' in name:
                ATCC_id = ATCC_id.replace("MAY", "MYA")
            if 'D' in name:
                ATCC_id = ATCC_id.split("D")[0]
            if 'T' in name:
                ATCC_id = ATCC_id.split("T")[0]
            if 's' in name:
                ATCC_id = ATCC_id.split("s")[0]
            if " " in name:
                ATCC_id = ATCC_id.split(" ")[0]

        # 手动标记过 ATCC 的情况
        if genome_ID_to_species_first_name_dict.get(ATCC_id) is None:
            cleaned_data.append(line)
            marked_ATCC_IDs.add(ATCC_id)

        # 如果 species name 符合，那么是干净的数据
        elif genome_ID_to_species_first_name_dict[ATCC_id] in name:
            cleaned_data.append(line)

    cleaned_data = np.array(cleaned_data)

    wrong_ATCC_numbers = set(Evo_MIC_data_with_genome_embedding[:, 1]) - set(cleaned_data[:, 1])

    print(f'\n wrong strain names: {wrong_ATCC_numbers}')
    print(f'\n double marked_ATCC_IDs: {marked_ATCC_IDs}')

    print(f'\n original data length (no "*", no manual modification) {original_length}\n cleaned data length {len(cleaned_data)}\n')

    return cleaned_data


def cluster_species(path_to_phy_tree:Path, old_to_new_NCBI_taxonomy_map_path:Path, num_clusters:int=5):
    """
    通过 tree of life 来获得距离矩阵进而计算聚类
    :param path_to_phy_tree: 到进化树的 Path
    :param old_to_new_NCBI_taxonomy_map_path: 到 NCBI 命名分类规则新旧 map 的 json dict 文件
    :param num_clusters: 要把所有的 species 分成几类
    :return: list[[species in cluster 0], [species in cluster 1], ...]
    """
    tree = Phylo.read(path_to_phy_tree, "newick")
    species = np.array([clade.name for clade in tree.get_terminals()])

    # Tree 里的 name 都是已经被 NCBI 替换成更新的分类标准了，但是 ATCC 的可能还比较滞后总之就是不一样，现在需要替换回来
    # 读取手动处理好的 map 规则
    with open(old_to_new_NCBI_taxonomy_map_path, 'r', encoding='utf-8') as f:
        old_to_new_NCBI_taxonomy_map = json.load(f)
    # new_to_old_NCBI_taxonomy_map = dict(zip(list(old_to_new_NCBI_taxonomy_map.values()), list(old_to_new_NCBI_taxonomy_map.keys())))

    # 构建距离矩阵
    n = len(species)
    dist_matrix = np.zeros((n, n))
    for i in tqdm(range(n), desc=' computing distance matrix'):
        for j in range(i, n):
            dist_matrix[i, j] = tree.distance(species[i], species[j])

    dist_matrix = dist_matrix + dist_matrix.T

    # 训练得到聚类
    agg = AgglomerativeClustering(n_clusters=num_clusters, metric='precomputed', linkage='average')
    labels = agg.fit_predict(dist_matrix)
    labels_set = set(labels)

    # 这里树中的 species name 都是已经被 NCBI 替换成更新的分类标准了，但是 ATCC 的可能还比较滞后总之就是不一样，现在需要替换回来
    # 这里树中的 species name 为了正确创建树已经把 Serratia sp 替换成了 Prodigiosinella aquatilis，所以在提取数据时要记得换回来
    print(f'\n Mapping back species names:\n before mapping back species: "Serratia" in species: {"Serratia" in species}')
    for old_name, new_name in old_to_new_NCBI_taxonomy_map.items():
        species[species == new_name] = old_name
    print(f' after mapping back species: "Serratia" in species: {"Serratia" in species}, "Prodigiosinella aquatilis" replaced\n ')

    all_grouped_species = []
    for label in labels_set:
        indices = np.where(labels == label)[0]
        grouped_species = species[indices]
        all_grouped_species.append(grouped_species)

    return all_grouped_species


def get_ATCC_ID_to_species_name_map(ATCC_fasta_folder_path:Path):
    file_names = [f.name for f in ATCC_fasta_folder_path.iterdir() if f.is_file()]

    # ATCC_ID_to_species_names_map = {}

    ATCC_ID_list = []
    species_name_list = []

    for file_name in file_names:

        # 先获得这个 ATCC genome fasta 文件的 ATCC ID
        ATCC_id = file_name.split('.')[0].split('ATCC')[-1].strip()
        ATCC_id = ATCC_id.replace("_", " ").strip().replace(" ", "-")
        ATCC_ID_list.append(ATCC_id)

        # 然后获得这个 ATCC genome fasta 文件的 species name
        file_name = file_name.split('ATCC')[0]
        if 'subsp' in file_name.split('_'):
            file_name = file_name.split('subsp')[0]
        if 'pathovar' in file_name.split('_'):
            file_name = file_name.split('pathovar')[0]  # 带有 pathovar 和 var 的在 NCBI Taxonomy Browser 中都是识别不到的
        if 'var' in file_name.split('_'):
            file_name = file_name.split('var')[0]
        if 'sp' in file_name.split('_'):
            file_name = file_name.split('_sp')[0]
        species_name = file_name.replace('_', ' ').strip()
        species_name_list.append(species_name)

        # 存进这个 map 字典里
        # ATCC_ID_to_species_names_map[ATCC_id] = species_name

    ATCC_ID_to_species_names_map = dict(zip(ATCC_ID_list, species_name_list))
    species_names_to_ATCC_ID_map = {}

    ATCC_ID_list = np.array(ATCC_ID_list)
    species_name_list = np.array(species_name_list)

    for species_name in set(species_name_list):
        species_names_to_ATCC_ID_map[species_name] = ATCC_ID_list[species_name_list == species_name]

    return ATCC_ID_to_species_names_map, species_names_to_ATCC_ID_map


def get_original_strain_ID_to_species_name_map(original_text_emb_folder_path:Path):
    file_names = [f.name for f in original_text_emb_folder_path.iterdir() if f.is_file()]

    # ATCC_ID_to_species_names_map = {}

    strain_name_list = []
    species_name_list = []

    for file_name in file_names:

        # 先获得这个 ATCC genome fasta 文件的 ATCC ID
        strain_name = file_name.split('.pt')[0].replace('～', ' ').replace('^', '/')
        species_name = " ".join(strain_name.split(' ')[:2])
        strain_name_list.append(strain_name)
        species_name_list.append(species_name)

    strain_name_to_species_names_map = dict(zip(strain_name_list, species_name_list))
    species_names_to_strain_name_map = {}

    strain_name_list = np.array(strain_name_list)
    species_name_list = np.array(species_name_list)

    for species_name in set(species_name_list):
        species_names_to_strain_name_map[species_name] = strain_name_list[species_name_list == species_name]

    return strain_name_to_species_names_map, species_names_to_strain_name_map


def merge_dict(dict_1, dict_2):
    merged_dict = {}

    # 先将第一个字典中的内容全部添加到merged_dict中
    for key, value in dict_1.items():
        merged_dict[key] = list(value)  # 复制列表，防止原列表被修改

    # 遍历第二个字典
    for key, value in dict_2.items():
        if key in merged_dict:
            # 如果键已存在，则合并两个列表
            merged_dict[key].extend(value)
        else:
            # 如果键不存在，则直接添加
            merged_dict[key] = list(value)

    return merged_dict


def load_all_genome_embeddings(embeddings_folder_path, scale, device, desc_str):
    """
    返回一个 genome ID 到 Evo2 embedding 字典
    :param embeddings_folder_path: 保存 Genome 的 Evo2 embed 的文件夹路径
    :param scale: Evo2 的 embedding 量级大概在 1e-15 左右，和模型参数 1e-2 左右的量级差太多了，所以需要缩放匹配
    :param device: 提前将所有的 Evo2 embedding 载入到显存之中，减少加载时间
    :return: dict  e.g. {'25922': torch.tensor([...], dtype=torch.bfloat16), ...}
    """
    file_paths = [embeddings_folder_path / f.name for f in embeddings_folder_path.iterdir() if f.is_file()]
    embeddings_dict = {}
    for file_path in tqdm(file_paths, desc=f' loading {desc_str} embeddings ... '):
        embedding = torch.load(file_path).to(device)
        file_name = file_path.name.split('.')[0]
        if 'ATCC' in file_name:
            file_name = file_name.split('ATCC')[-1]
            components = file_name.split('_')[1:]
            if len(components) == 2:
                ID = '-'.join(components)
            else:
                ID = components[0]
        else:
            # 自己下载的情况
            ID = file_name
        embeddings_dict[ID] = embedding * scale

    return embeddings_dict


def load_text_wo_genome_embeddings(embeddings_folder_path, scale, device, desc_str):
    """
    返回一个 genome ID 到 Evo2 embedding 字典
    :param embeddings_folder_path: 保存 Genome 的 Evo2 embed 的文件夹路径
    :param scale: Evo2 的 embedding 量级大概在 1e-15 左右，和模型参数 1e-2 左右的量级差太多了，所以需要缩放匹配
    :param device: 提前将所有的 Evo2 embedding 载入到显存之中，减少加载时间
    :return: dict  e.g. {'25922': torch.tensor([...], dtype=torch.bfloat16), ...}
    """
    file_paths = [embeddings_folder_path / f.name for f in embeddings_folder_path.iterdir() if f.is_file()]
    embeddings_dict = {}
    for file_path in tqdm(file_paths, desc=f' loading {desc_str} embeddings ... '):
        embedding = torch.load(file_path).to(device)
        file_name = file_path.name.split('.pt')[0]
        strain_name = file_name.replace('～', ' ').replace('^', '/')
        embeddings_dict[strain_name] = embedding * scale

    return embeddings_dict
